{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = True):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state'] \n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label:\n",
    "            def label_transformer(category):\n",
    "                if category == 'Normal':\n",
    "                    return 0\n",
    "                elif category in ['Reconnaissance', 'Analysis', 'Fuzzers', 'Shellcode', 'Generic']:\n",
    "                    return 0\n",
    "                elif category in ['Backdoor', 'DoS', 'Exploits', 'Worms']:\n",
    "                    return 1\n",
    "\n",
    "            df['label'] = df['attack_cat'].apply(label_transformer)\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreProcessPipeline(label_encode=True, process_label=True)\n",
    "train_df = pipeline.transform(train_df)\n",
    "test_df = pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "train_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316GP2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark data frame\n",
    "\n",
    "spark_train_df = spark.createDataFrame(train_df)\n",
    "spark_test_df = spark.createDataFrame(test_df)\n",
    "\n",
    "# Define the feature columns\n",
    "feature_columns = spark_train_df.columns[:-1]  # Exclude the \"label\" column\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "feature_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "train = feature_assembler.transform(spark_train_df)\n",
    "test = feature_assembler.transform(spark_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler(inputCols=feature_columns, outputCol=\"scaled_features\")\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "scaler_model = scaler.fit(train)\n",
    "train_scaled = scaler_model.transform(train)\n",
    "test_scaled = scaler_model.transform(test)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(samplingStrategy=\"auto\", k=5, percentage=100)\n",
    "\n",
    "# Apply SMOTE to the scaled training data\n",
    "train_resampled = smote.fit(train_scaled).transform(train_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for SVM\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM model\n",
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
    "svm_model = svm.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction \n",
    "\n",
    "predictions = svm_model.transform(test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "roc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "roc_score = roc_evaluator.evaluate(predictions)\n",
    "print(\"Area under ROC = %g\" % roc_score)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "pr_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
    "pr_score = pr_evaluator.evaluate(predictions)\n",
    "print(\"Area under PR = %g\" % pr_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With hyper parameter\n",
    "\n",
    "# Train an SVM model with different hyperparameters\n",
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100, regParam=0.01)\n",
    "svm_model = svm.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
