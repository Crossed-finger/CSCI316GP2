{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Assignment 2 \n",
    "\n",
    "### Authors: \n",
    "-  Chin Yee Wan \n",
    "-  Darrel Koh\n",
    "-  Nguyen Gia Khanh \n",
    "-  Ngo Vu Anh\t\n",
    "\n",
    "### Main Steps\n",
    "1.  Data Preprocessing \n",
    "-   Read in as SPARK dataframe for data preprocessing\n",
    "-   Convert to Pandas dataframe for data exploration\n",
    "2.  Data Exploration\n",
    "3.  Data Modelling\n",
    "4.  Data Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and Visualise the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                            .appName(\"CSCI316GP2\")\\\n",
    "                            .config(\"spark.sql.files.maxPartitionBytes\", \"1000000\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(train_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom pipeline for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = 'Binary', smote = False):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "        self.smote = smote\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state', 'attack_cat']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label == 'Binary':\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "        else:             \n",
    "            df['attack_cat'], df['label'] = df['label'], df['attack_cat']\n",
    "            df.drop('attack_cat', axis=1, inplace=True)   \n",
    "\n",
    "        if self.smote:\n",
    "            # Separate features and labels\n",
    "            X = df.drop('label', axis=1)\n",
    "            y = df['label']\n",
    "\n",
    "            # Apply SMOTE for oversampling\n",
    "            smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "            # Convert NumPy arrays back to Pandas DataFrames\n",
    "            X_resampled_df = pd.DataFrame(data=X_resampled, columns=X.columns)\n",
    "            y_resampled_df = pd.DataFrame(data=y_resampled, columns=['label'])\n",
    "\n",
    "            # Concatenate the features and label columns into a single DataFrame\n",
    "            df = pd.concat([X_resampled_df, y_resampled_df], axis=1)     \n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Khanh Nguyen\n",
    "Name: PySpark Dataframe Pipeline\n",
    "Description:\n",
    "    This class is used to create a pipeline for PySpark dataframe, accept 2 boolean parameter: smote & standardize.\n",
    "    Features \n",
    "        (Default)\n",
    "        - Resample: Resample the dataframe\n",
    "        - Vectorize: Vectorize the dataframe\n",
    "        (activate by setting the parameter to True):\n",
    "        - SMOTE: Oversampling the minority class\n",
    "        - Standardize: Standardize the dataframe using z-score\n",
    "'''\n",
    "class SparkDFPipeline:\n",
    "    def __init__(self, standardize=False):\n",
    "        self.standardize = standardize\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, train_df, test_df):      \n",
    "        if self.standardize:\n",
    "            # Standardize the df\n",
    "\n",
    "            # Resample the df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "\n",
    "            exclude = ['proto', 'service', 'state']\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            selected_columns = [col for col in input_columns if col not in exclude]\n",
    "            # Vectorize the df\n",
    "            assembler = VectorAssembler(inputCols=selected_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            test_df = assembler.transform(test_df)\n",
    "\n",
    "            # Standardize the df\n",
    "            scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "            scaler_model = scaler.fit(train_df)\n",
    "            train_df = scaler_model.transform(train_df)\n",
    "\n",
    "            scaler_model = scaler.fit(test_df)\n",
    "            test_df = scaler_model.transform(test_df)\n",
    "            test_df = test_df.drop('features')\n",
    "            train_df = train_df.drop('features')\n",
    "            \n",
    "            # put back the categorical columns\n",
    "            input_cols = ['scaled_features', 'proto', 'service', 'state']\n",
    "            output_col = \"features\"\n",
    "            assembler1 = VectorAssembler(inputCols=input_cols, outputCol=output_col)\n",
    "            train_df = assembler1.transform(train_df)\n",
    "            test_df = assembler1.transform(test_df)\n",
    "\n",
    "            # return result\n",
    "            test_df = test_df.select('features', 'label')\n",
    "            train_df = train_df.select('features', 'label')\n",
    "        else:\n",
    "            # Normal vectorize df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            train_df = train_df.select('features', 'label')\n",
    "            test_df = assembler.transform(test_df)\n",
    "              \n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_weighted_logistic_regression(train_df):\n",
    "    # Calculate class frequencies\n",
    "    class_frequencies = train_df.groupBy(\"label\").count()\n",
    "\n",
    "    # Calculate class weights\n",
    "    total_samples = train_df.count()\n",
    "    class_frequencies = class_frequencies.withColumn(\"weight\", total_samples / (class_frequencies[\"count\"] * class_frequencies.count()))\n",
    "\n",
    "    # Join the weights with the training data\n",
    "    train_with_weights = train_df.join(class_frequencies, on=\"label\")\n",
    "\n",
    "    # Add a constant column for weight if it doesn't exist\n",
    "    if \"weight\" not in train_with_weights.columns:\n",
    "        train_with_weights = train_with_weights.withColumn(\"weight\", lit(1.0))\n",
    "\n",
    "    # Create a VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=['features'], outputCol='assembled_features')\n",
    "\n",
    "    # Define the Logistic Regression model with class weights\n",
    "    lr_weighted = LogisticRegression(featuresCol='assembled_features', labelCol='label', maxIter=10, weightCol='weight')\n",
    "\n",
    "    # Create a pipeline with the defined stages\n",
    "    pipeline_weighted = Pipeline(stages=[assembler, lr_weighted])\n",
    "\n",
    "    # Fit the pipeline on your training data with class weights\n",
    "    model_weighted = pipeline_weighted.fit(train_with_weights)\n",
    "    \n",
    "    return model_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your class labels\n",
    "class_labels = ['Normal', 'Generic', 'Exploits', 'Fuzzers', 'DoS', 'Reconnaissance', 'Analysis', 'Backdoor', 'Shellcode', 'Worms']\n",
    "\n",
    "def evaluate_model(model, val_data, model_name, process_label):\n",
    "    # Make predictions on the validation data\n",
    "    predictions = model.transform(val_data)\n",
    "\n",
    "    if process_label == 'Multi':\n",
    "        acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        accuracy = acc_evaluator.evaluate(predictions)\n",
    "\n",
    "        f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        f1_score = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "        # AUC_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "        # AUC_score = AUC_evaluator.evaluate(predictions)\n",
    "\n",
    "        # AUPR_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
    "        # AUPR_score = AUPR_evaluator.evaluate(predictions)\n",
    "\n",
    "        precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "        precision_score = precision_evaluator.evaluate(predictions)\n",
    "\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "        recall_score = recall_evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Convert Spark DataFrames to Pandas DataFrames for visualization\n",
    "        y_true_pd = predictions.select('label').toPandas()\n",
    "        y_pred_pd = predictions.select('prediction').toPandas()\n",
    "\n",
    "        # Generate the confusion matrix\n",
    "        cm = confusion_matrix(y_true_pd['label'], y_pred_pd['prediction'])\n",
    "\n",
    "        # Get the predicted counts for each class label\n",
    "        predicted_counts = y_pred_pd['prediction'].value_counts()\n",
    "\n",
    "        # Create a dictionary to store the counts of each class label\n",
    "        class_counts = {label: 0 for label in class_labels}\n",
    "\n",
    "        # Fill in the dictionary with actual predicted counts where available\n",
    "        for key, count in predicted_counts.items():\n",
    "            class_counts[class_labels[int(key)]] = count\n",
    "\n",
    "        # Convert the dictionary values to a list\n",
    "        predicted_counts_list = [class_counts[label] for label in class_labels]\n",
    "\n",
    "        # Display the confusion matrix as a heatmap with sorted class labels and counts\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels[::-1], yticklabels=class_labels)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(model_name)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Create a dictionary of model evaluation metrics\n",
    "        eval_metrics = {\n",
    "            'Accuracy': accuracy\n",
    "            , 'F1 Score': f1_score\n",
    "            # , 'Area Under ROC': AUC_score\n",
    "            # , 'Area Under PR': AUPR_score\n",
    "            , 'Precision': precision_score\n",
    "            , 'Recall': recall_score\n",
    "        }\n",
    "\n",
    "    elif process_label == 'Binary':\n",
    "        # Evaluate the model using a BinaryClassificationEvaluator for AUC\n",
    "        auc_evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "        auc = auc_evaluator.evaluate(predictions)\n",
    "        # Evaluate the model using a BinaryClassificationEvaluator for AUC\n",
    "        auc_evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "        auc = auc_evaluator.evaluate(predictions)\n",
    "\n",
    "        # Evaluate the model using a BinaryClassificationEvaluator for areaUnderPR\n",
    "        pr_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderPR')\n",
    "        area_under_pr = pr_evaluator.evaluate(predictions)\n",
    "\n",
    "        \n",
    "\n",
    "        # Calculate additional metrics\n",
    "        sensitivity = predictions.filter(\"label = 1 and prediction = 1\").count() / predictions.filter(\"label = 1\").count()\n",
    "        specificity = predictions.filter(\"label = 0 and prediction = 0\").count() / predictions.filter(\"label = 0\").count()\n",
    "        precision = predictions.filter(\"prediction = 1\").count() / predictions.filter(\"prediction = 1 or prediction = 0\").count()\n",
    "        recall = sensitivity\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        accuracy = (predictions.filter(\"label = prediction\").count()) / predictions.count()\n",
    "        # youdens_j = sensitivity + specificity - 1\n",
    "        # balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "        # Convert Spark DataFrames to Pandas DataFrames for visualization\n",
    "        y_true_pd = predictions.select('label').toPandas()\n",
    "        y_pred_pd = predictions.select('prediction', 'probability').toPandas()\n",
    "        \n",
    "        # Convert prediction probabilities to binary predictions\n",
    "        y_pred_binary = [1 if prob[1] >= 0.5 else 0 for prob in y_pred_pd['probability']]\n",
    "\n",
    "        # Generate the confusion matrix\n",
    "        cm = confusion_matrix(y_true_pd['label'], y_pred_binary)\n",
    "\n",
    "        # Display the confusion matrix as a heatmap\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-default', 'Default'], \n",
    "                    yticklabels=['Non-default', 'Default'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(model_name)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Create a dictionary of model evaluation metrics\n",
    "        eval_metrics = {\n",
    "            'AUC': auc,\n",
    "            'AreaUnderPR': area_under_pr,\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Specificity': specificity,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1_score,\n",
    "            'Accuracy': accuracy,\n",
    "            # \"Youden's J Index\": youdens_j,\n",
    "            # 'Balanced Accuracy': balanced_accuracy\n",
    "        }\n",
    "\n",
    "    return eval_metrics\n",
    "\n",
    "\n",
    "# Sample usage\n",
    "# # Evaluate models using the evaluate_model function\n",
    "# evaluation_results_default = evaluate_model(model_default, test, 'Default Model',process_label)  # Use the 'test' dataset\n",
    "# evaluation_results_best_tuned = evaluate_model(best_tuned_model, FT_test, 'Best-Tuned Model', process_label)  # Use the 'FT_test' dataset\n",
    "# evaluation_results_weighted = evaluate_model(weighted_lr_model, FT_test, 'Weighted Model', process_label)  # Use the 'FT_test' dataset\n",
    "\n",
    "# # Print evaluation results for all models side by side\n",
    "# print(\"Evaluation Results:\")\n",
    "# print(f\"{'Metric':<20}{'Default Model':<20}{'Best-Tuned Model':<20}{'Weighted Model':<20}\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# for metric in evaluation_results_default.keys():\n",
    "#     default_value = evaluation_results_default[metric]\n",
    "#     best_tuned_value = evaluation_results_best_tuned[metric]\n",
    "#     weighted_value = evaluation_results_weighted[metric]\n",
    "#     print(f\"{metric:<20}{default_value:<20.6f}{best_tuned_value:<20.6f}{weighted_value:<20.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore train_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change either 'Binary' or 'Multi' according to Classification use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_label = 'Binary'  # or 'Binary' depending on use case\n",
    "\n",
    "if process_label == 'Multi':\n",
    "    pipeline_train = PreProcessPipeline(label_encode=True, process_label='Multi')\n",
    "    pipeline_test = PreProcessPipeline(label_encode=True, process_label='Multi')\n",
    "elif process_label == 'Binary':\n",
    "    pipeline_train = PreProcessPipeline(label_encode=True, process_label='Binary')\n",
    "    pipeline_test = PreProcessPipeline(label_encode=True, process_label='Binary')\n",
    "\n",
    "# Transform train and test data using the appropriate pipeline\n",
    "train_df = pipeline_train.transform(train_df)\n",
    "test_df = pipeline_test.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plot countplot for column1\n",
    "sns.countplot(data=train_df, x='label', palette=\"Set1\", ax=axes[0])\n",
    "axes[0].set_title(\"Label Plot for train df\")\n",
    "axes[0].set_xlabel(\"Column 1\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot countplot for column2\n",
    "sns.countplot(data=test_df, x='label', palette=\"Set2\", ax=axes[1])\n",
    "axes[1].set_title(\"Label Plot for test df\")\n",
    "axes[1].set_xlabel(\"Column 2\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "train_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Coorelation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - Correlation matrix\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = train_df.corr()\n",
    "\n",
    "# Select the correlation values with 'label', label here means attack_cat\n",
    "target_corr = corr_matrix['label']\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature ranking in corr order \n",
    "\n",
    "# Calculate the absolute correlation values with the 'label'\n",
    "target_corr_abs = corr_matrix['label'].abs()\n",
    "\n",
    "# Sort the correlation values in descending order\n",
    "sorted_corr = target_corr_abs.sort_values(ascending=False)\n",
    "\n",
    "# Print the sorted correlation values and their corresponding attributes\n",
    "for attribute, correlation in target_corr_abs.items():\n",
    "    print(f\"{attribute}: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas DF to Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparktrain_df = spark.createDataFrame(train_df)\n",
    "sparktest_df = spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature enabler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SparkDFPipeline(standardize=True)\n",
    "train, test = pipeline.transform(sparktrain_df, sparktest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SparkDFPipeline(standardize=True)\n",
    "FT_train, FT_test = pipeline.transform(sparktrain_df, sparktest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training\n",
    "- Select machine learning models (Logistic Regression , Decision Tree, Random Forest, Multilayer perceptron).\n",
    "- Split the data into training and validation sets.\n",
    "- Train the selected models using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for default model\n",
    "lr_default = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "pipeline_default = Pipeline(stages=[lr_default])\n",
    "model_default = pipeline_default.fit(train)  # Use the 'train' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with the defined stages\n",
    "pipeline = Pipeline(stages=[lr_default])\n",
    "\n",
    "# Define the ParamGrid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr_default.maxIter, [10, 20, 30]) \\\n",
    "    .addGrid(lr_default.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up the appropriate evaluator based on process_label\n",
    "if process_label == 'Multi':\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "elif process_label == 'Binary':\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# Instantiate CrossValidator with the pipeline and paramGrid\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Fit the CrossValidator on your training data\n",
    "best_tuned_model = cv.fit(FT_train).bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-weighted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_lr_model = train_weighted_logistic_regression(FT_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models using the evaluate_model function\n",
    "evaluation_results_default = evaluate_model(model_default, test, 'Default Model',process_label)  # Use the 'test' dataset\n",
    "evaluation_results_best_tuned = evaluate_model(best_tuned_model, FT_test, 'Best-Tuned Model', process_label)  # Use the 'FT_test' dataset\n",
    "evaluation_results_weighted = evaluate_model(weighted_lr_model, FT_test, 'Weighted Model', process_label)  # Use the 'FT_test' dataset\n",
    "\n",
    "# Print evaluation results for all models side by side\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'Default Model':<20}{'Best-Tuned Model':<20}{'Weighted Model':<20}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric in evaluation_results_default.keys():\n",
    "    default_value = evaluation_results_default[metric]\n",
    "    best_tuned_value = evaluation_results_best_tuned[metric]\n",
    "    weighted_value = evaluation_results_weighted[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20.6f}{best_tuned_value:<20.6f}{weighted_value:<20.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SparkDFPipeline(standardize=False)\n",
    "train, test = pipeline.transform(train_df, test_df)\n",
    "\n",
    "# Train a RandomForest model\n",
    "default_rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=42)\n",
    "\n",
    "# Create a pipeline\n",
    "default_pipeline = Pipeline(stages=[default_rf])\n",
    "\n",
    "# Train the pipeline\n",
    "default_model = default_pipeline.fit(train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "default_predictions = default_model.transform(test)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "default_accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "default_accuracy = default_accuracy_evaluator.evaluate(default_predictions)\n",
    "print(\"Accuracy = %g\" % default_accuracy)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "default_f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "default_f1_score = default_f1_evaluator.evaluate(default_predictions)\n",
    "print(\"F1 Score = %g\" % default_f1_score)\n",
    "\n",
    "# Create an evaluator for precision\n",
    "default_precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "default_precision = default_precision_evaluator.evaluate(default_predictions)\n",
    "print(\"Precision = %g\" % default_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SparkDFPipeline(standardize=True)\n",
    "FT_train, FT_test = pipeline.transform(train_df, test_df)\n",
    "\n",
    "# Train a RandomForest model\n",
    "tuned_rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=42)\n",
    "tuned_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(tuned_rf.numTrees, [10,20,30]) \\\n",
    "            .addGrid(tuned_rf.maxDepth, [5,6,8]) \\\n",
    "            .addGrid(tuned_rf.impurity, ['gini']) \\\n",
    "            .build()\n",
    "\n",
    "# Create a CrossValidator instance\n",
    "crossval = CrossValidator(estimator=tuned_rf,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=tuned_evaluator,\n",
    "                          numFolds=3)  # Number of cross-validation folds\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cv_model = crossval.fit(FT_train)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Get the best parameters\n",
    "best_numTrees = best_model.getOrDefault('numTrees')\n",
    "best_maxDepth = best_model.getOrDefault('maxDepth')\n",
    "best_impurity = best_model.getOrDefault('impurity')\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best numTrees:\", best_numTrees)\n",
    "print(\"Best maxDepth:\", best_maxDepth)\n",
    "print(\"Best impurity:\", best_impurity)\n",
    "\n",
    "# Make predictions on the test data\n",
    "best_predictions = best_model.transform(FT_test)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "best_accuracy = accuracy_evaluator.evaluate(best_predictions)\n",
    "print(\"Accuracy = %g\" % best_accuracy)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "best_f1_score = f1_evaluator.evaluate(best_predictions)\n",
    "print(\"F1 Score = %g\" % best_f1_score)\n",
    "\n",
    "# Create an evaluator for precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "best_precision = precision_evaluator.evaluate(best_predictions)\n",
    "print(\"Precision = %g\" % best_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models using the evaluate_model function\n",
    "evaluation_results_default = evaluate_model(default_model, test, 'Default Model')  # Use the 'test' dataset\n",
    "\n",
    "# Evaluate models using the evaluate_model function\n",
    "evaluation_results_tuned = evaluate_model(best_model, FT_test, 'Fine-Tuned Model')  # Use the 'test' dataset\n",
    "\n",
    "# Print evaluation results for both models side by side\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'Default Model':<20}{'Fine-Tuned Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_default.keys():\n",
    "    default_value = evaluation_results_default[metric]\n",
    "    tuned_value = evaluation_results_tuned[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20}{tuned_value:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comparison between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
