{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Assignment 2 \n",
    "\n",
    "### Authors: \n",
    "-  Chin Yee Wan \n",
    "-  Darrel Koh\n",
    "-  Nguyen Gia Khanh \n",
    "-  Ngo Vu Anh\t\n",
    "\n",
    "### Main Steps\n",
    "1.  Data Preprocessing \n",
    "-   Read in as SPARK dataframe for data preprocessing\n",
    "-   Convert to Pandas dataframe for data exploration\n",
    "2.  Data Exploration\n",
    "3.  Data Modelling\n",
    "4.  Data Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and Visualise the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                            .appName(\"CSCI316GP2\")\\\n",
    "                            .config(\"spark.sql.files.maxPartitionBytes\", \"1000000\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(train_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom pipeline for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = True):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label:\n",
    "            def label_transformer(category):\n",
    "                if category == 'Normal':\n",
    "                    return 0\n",
    "                elif category in ['Reconnaissance', 'Analysis', 'Fuzzers', 'Shellcode', 'Generic']:\n",
    "                    return 0\n",
    "                elif category in ['Backdoor', 'DoS', 'Exploits', 'Worms']:\n",
    "                    return 1\n",
    "\n",
    "            df['label'] = df['attack_cat'].apply(label_transformer)\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Khanh Nguyen\n",
    "Name: PySpark Dataframe Pipeline\n",
    "Description:\n",
    "    This class is used to create a pipeline for PySpark dataframe, accept 2 boolean parameter: smote & standardize.\n",
    "    Features \n",
    "        (Default)\n",
    "        - Resample: Resample the dataframe\n",
    "        - Vectorize: Vectorize the dataframe\n",
    "        (activate by setting the parameter to True):\n",
    "        - SMOTE: Oversampling the minority class\n",
    "        - Standardize: Standardize the dataframe using z-score\n",
    "'''\n",
    "class SparkDFPipeline:\n",
    "    def __init__(self, smote=False, standardize=False):\n",
    "        self.smote = smote\n",
    "        self.standardize = standardize\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, train_df, test_df):\n",
    "        if self.smote:\n",
    "            majority = train_df.filter(col('label') == 0)\n",
    "            minority = train_df.filter(col('label') == 1)\n",
    "\n",
    "            majority_count = majority.count()\n",
    "            minority_count = minority.count()\n",
    "\n",
    "            ratio = int(majority_count / minority_count)\n",
    "            sample_num = int(ratio * minority_count) - minority_count\n",
    "            sample = minority.sample(True, sample_num / minority_count, seed=42)\n",
    "            balanced_sample = minority.union(sample)\n",
    "            train_df = majority.union(balanced_sample).orderBy('label')\n",
    "        \n",
    "        if self.standardize:\n",
    "            # Standardize the df\n",
    "\n",
    "            # Resample the df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "\n",
    "            exclude = ['proto', 'service', 'state']\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            selected_columns = [col for col in input_columns if col not in exclude]\n",
    "\n",
    "            # Vectorize the df\n",
    "            assembler = VectorAssembler(inputCols=selected_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            test_df = assembler.transform(test_df)\n",
    "\n",
    "            # Standardize the df\n",
    "            scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "            scaler_model = scaler.fit(train_df)\n",
    "            train_df = scaler_model.transform(train_df)\n",
    "\n",
    "            scaler_model = scaler.fit(test_df)\n",
    "            test_df = scaler_model.transform(test_df)\n",
    "            test_df = test_df.drop('features')\n",
    "            train_df = train_df.drop('features')\n",
    "            \n",
    "            # put back the categorical columns\n",
    "            input_cols = ['scaled_features', 'proto', 'service', 'state']\n",
    "            output_col = \"features\"\n",
    "            assembler1 = VectorAssembler(inputCols=input_cols, outputCol=output_col)\n",
    "            train_df = assembler1.transform(train_df)\n",
    "            test_df = assembler1.transform(test_df)\n",
    "\n",
    "            # return result\n",
    "            test_df = test_df.select('features', 'label')\n",
    "            train_df = train_df.select('features', 'label')\n",
    "        else:\n",
    "            # Normal vectorize df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            test_df = assembler.transform(test_df)\n",
    "            train_df = train_df.select('features', 'label')\n",
    "            \n",
    "              \n",
    "        return train_df, test_df\n",
    "\n",
    "\n",
    "## Pipeline trigger example\n",
    "# pipeline = SparkDFPipeline(smote=False, standardize=False)\n",
    "# train, test = pipeline.transform(sparktrain_df, sparktest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_data, model_name):\n",
    "    # Make predictions on the validation data\n",
    "    predictions = model.transform(val_data)\n",
    "\n",
    "    # Evaluate the model using a BinaryClassificationEvaluator for AUC\n",
    "    auc_evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Evaluate the model using a BinaryClassificationEvaluator for areaUnderPR\n",
    "    pr_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderPR')\n",
    "    area_under_pr = pr_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    sensitivity = predictions.filter(\"label = 1 and prediction = 1\").count() / predictions.filter(\"label = 1\").count()\n",
    "    specificity = predictions.filter(\"label = 0 and prediction = 0\").count() / predictions.filter(\"label = 0\").count()\n",
    "    precision = predictions.filter(\"prediction = 1\").count() / predictions.filter(\"prediction = 1 or prediction = 0\").count()\n",
    "    recall = sensitivity\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (predictions.filter(\"label = prediction\").count()) / predictions.count()\n",
    "    # youdens_j = sensitivity + specificity - 1\n",
    "    # balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "    # Convert Spark DataFrames to Pandas DataFrames for visualization\n",
    "    y_true_pd = predictions.select('label').toPandas()\n",
    "    y_pred_pd = predictions.select('prediction', 'probability').toPandas()\n",
    "    \n",
    "    # Convert prediction probabilities to binary predictions\n",
    "    y_pred_binary = [1 if prob[1] >= 0.5 else 0 for prob in y_pred_pd['probability']]\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_true_pd['label'], y_pred_binary)\n",
    "\n",
    "    # Display the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-default', 'Default'], \n",
    "                yticklabels=['Non-default', 'Default'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Create a dictionary of model evaluation metrics\n",
    "    eval_metrics = {\n",
    "        'AUC': auc,\n",
    "        'AreaUnderPR': area_under_pr,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1_score,\n",
    "        'Accuracy': accuracy,\n",
    "        # \"Youden's J Index\": youdens_j,\n",
    "        # 'Balanced Accuracy': balanced_accuracy\n",
    "    }\n",
    "\n",
    "    return eval_metrics\n",
    "\n",
    "\n",
    "# # Sample usage \n",
    "# # Evaluate models using the evaluate_model function\n",
    "# evaluation_results_default = evaluate_model(model_default, test, 'Default Model')  # Use the 'test' dataset\n",
    "\n",
    "# # Evaluate models using the evaluate_model function\n",
    "# evaluation_results_tuned = evaluate_model(model_tuned, FT_test, 'Fine-Tuned Model')  # Use the 'test' dataset\n",
    "\n",
    "# # Print evaluation results for both models side by side\n",
    "\n",
    "# print(\"Evaluation Results:\")\n",
    "# print(f\"{'Metric':<20}{'Default Model':<20}{'Fine-Tuned Model':<20}\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# for metric in evaluation_results_default.keys():\n",
    "#     default_value = evaluation_results_default[metric]\n",
    "#     tuned_value = evaluation_results_tuned[metric]\n",
    "#     print(f\"{metric:<20}{default_value:<20}{tuned_value:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore train_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreProcessPipeline(label_encode=True, process_label=True)\n",
    "train_df = pipeline.transform(train_df)\n",
    "test_df = pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "train_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas DF to Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparktrain_df = spark.createDataFrame(train_df)\n",
    "sparktest_df = spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training\n",
    "- Select machine learning models (Logistic Regression , Decision Tree, Random Forest, Multilayer perceptron).\n",
    "- Split the data into training and validation sets.\n",
    "- Train the selected models using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SparkDFPipeline(standardize=False)\n",
    "train, test = pipeline.transform(train_df, test_df)\n",
    "\n",
    "# Train a RandomForest model\n",
    "default_rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=42)\n",
    "\n",
    "# Create a pipeline\n",
    "default_pipeline = Pipeline(stages=[default_rf])\n",
    "\n",
    "# Train the pipeline\n",
    "default_model = default_pipeline.fit(train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "default_predictions = default_model.transform(test)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "default_accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "default_accuracy = default_accuracy_evaluator.evaluate(default_predictions)\n",
    "print(\"Accuracy = %g\" % default_accuracy)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "default_f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "default_f1_score = default_f1_evaluator.evaluate(default_predictions)\n",
    "print(\"F1 Score = %g\" % default_f1_score)\n",
    "\n",
    "# Create an evaluator for precision\n",
    "default_precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "default_precision = default_precision_evaluator.evaluate(default_predictions)\n",
    "print(\"Precision = %g\" % default_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SparkDFPipeline(standardize=True)\n",
    "FT_train, FT_test = pipeline.transform(train_df, test_df)\n",
    "\n",
    "# Train a RandomForest model\n",
    "tuned_rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", seed=42)\n",
    "tuned_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(tuned_rf.numTrees, [10,20,30]) \\\n",
    "            .addGrid(tuned_rf.maxDepth, [5,6,8]) \\\n",
    "            .addGrid(tuned_rf.impurity, ['gini']) \\\n",
    "            .build()\n",
    "\n",
    "# Create a CrossValidator instance\n",
    "crossval = CrossValidator(estimator=tuned_rf,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=tuned_evaluator,\n",
    "                          numFolds=3)  # Number of cross-validation folds\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cv_model = crossval.fit(FT_train)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Get the best parameters\n",
    "best_numTrees = best_model.getOrDefault('numTrees')\n",
    "best_maxDepth = best_model.getOrDefault('maxDepth')\n",
    "best_impurity = best_model.getOrDefault('impurity')\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best numTrees:\", best_numTrees)\n",
    "print(\"Best maxDepth:\", best_maxDepth)\n",
    "print(\"Best impurity:\", best_impurity)\n",
    "\n",
    "# Make predictions on the test data\n",
    "best_predictions = best_model.transform(FT_test)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "best_accuracy = accuracy_evaluator.evaluate(best_predictions)\n",
    "print(\"Accuracy = %g\" % best_accuracy)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "best_f1_score = f1_evaluator.evaluate(best_predictions)\n",
    "print(\"F1 Score = %g\" % best_f1_score)\n",
    "\n",
    "# Create an evaluator for precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "best_precision = precision_evaluator.evaluate(best_predictions)\n",
    "print(\"Precision = %g\" % best_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models using the evaluate_model function\n",
    "evaluation_results_default = evaluate_model(default_model, test, 'Default Model')  # Use the 'test' dataset\n",
    "\n",
    "# Evaluate models using the evaluate_model function\n",
    "evaluation_results_tuned = evaluate_model(best_model, FT_test, 'Fine-Tuned Model')  # Use the 'test' dataset\n",
    "\n",
    "# Print evaluation results for both models side by side\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'Default Model':<20}{'Fine-Tuned Model':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in evaluation_results_default.keys():\n",
    "    default_value = evaluation_results_default[metric]\n",
    "    tuned_value = evaluation_results_tuned[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20}{tuned_value:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comparison between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
