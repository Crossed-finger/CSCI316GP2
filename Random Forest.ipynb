{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline for dat pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = True):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label:\n",
    "            def label_transformer(category):\n",
    "                if category == 'Normal':\n",
    "                    return 0\n",
    "                elif category in ['Reconnaissance', 'Analysis', 'Fuzzers', 'Shellcode', 'Generic']:\n",
    "                    return 0\n",
    "                elif category in ['Backdoor', 'DoS', 'Exploits', 'Worms']:\n",
    "                    return 1\n",
    "\n",
    "            df['label'] = df['attack_cat'].apply(label_transformer)\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreProcessPipeline(label_encode=True, process_label=True)\n",
    "\n",
    "# Transform both training and test data\n",
    "train_df = pipeline.transform(train_df)\n",
    "test_df = pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "train_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316GP2\").getOrCreate()\n",
    "\n",
    "# Set the random seed\n",
    "seed = 42\n",
    "spark.conf.set(\"spark.seed\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the preprocessed training data into a Spark DataFrame\n",
    "spark_train_df = spark.createDataFrame(train_df)\n",
    "spark_test_df = spark.createDataFrame(test_df)\n",
    "spark_train_df.show()\n",
    "spark_test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature columns\n",
    "feature_columns = spark_train_df.columns[:-1]  # Exclude the \"label\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features into a single vector column\n",
    "feature_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "spark_train_df = feature_assembler.transform(spark_train_df)\n",
    "spark_test_df = feature_assembler.transform(spark_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the label column\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(spark_train_df)\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(spark_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[label_indexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline\n",
    "model = pipeline.fit(spark_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(spark_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluator for accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "print(\"F1 Score = %g\" % f1_score)\n",
    "\n",
    "# Create an evaluator for precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "print(\"Precision = %g\" % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Create a CrossValidator instance\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=accuracy_evaluator,\n",
    "                          numFolds=3)  # Number of cross-validation folds\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cv_model = crossval.fit(spark_train_df)\n",
    "\n",
    "best_model = cv_model.bestModel\n",
    "best_predictions = best_model.transform(spark_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluator for accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "best_accuracy = accuracy_evaluator.evaluate(best_predictions)\n",
    "print(\"Accuracy = %g\" % best_accuracy)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "best_f1_score = f1_evaluator.evaluate(best_predictions)\n",
    "print(\"F1 Score = %g\" % best_f1_score)\n",
    "\n",
    "# Create an evaluator for precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "best_precision = precision_evaluator.evaluate(best_predictions)\n",
    "print(\"Precision = %g\" % best_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
