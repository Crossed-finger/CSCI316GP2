{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Assignment 2 \n",
    "\n",
    "### Authors: \n",
    "-  Chin Yee Wan \n",
    "-  Darrel Koh\n",
    "-  Nguyen Gia Khanh \n",
    "-  Ngo Vu Anh\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and Visualise the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316GP2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(train_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom pipeline for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = True):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label:\n",
    "            def label_transformer(category):\n",
    "                if category == 'Normal':\n",
    "                    return 0\n",
    "                elif category in ['Reconnaissance', 'Analysis', 'Fuzzers', 'Shellcode', 'Generic']:\n",
    "                    return 0\n",
    "                elif category in ['Backdoor', 'DoS', 'Exploits', 'Worms']:\n",
    "                    return 1\n",
    "\n",
    "            df['label'] = df['attack_cat'].apply(label_transformer)\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Spark to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrames to Pandas\n",
    "# train_df = train_df.toPandas()\n",
    "# test_df_pandas = test_df.toPandas()\n",
    "# features_df_pandas = features_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Features set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df.head(20)\n",
    "# features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "# train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of zeros in each column:\n",
    "zero_counts = {}\n",
    "\n",
    "for column in train_df.columns:\n",
    "    zero_counts[column] = (train_df[column] == \"0\").sum()\n",
    "    \n",
    "# Create a DataFrame from the zero_counts dictionary\n",
    "zero_counts_df = pd.DataFrame(list(zero_counts.items()), columns=['Column', 'Zero Count'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(zero_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 Objects that will require Encoding\n",
    "- proto \n",
    "- service\n",
    "- state\n",
    "- attack_cat (1 of target variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proto Attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the unique values of the column 'proto' in the dataframe 'train_df' \n",
    "unique_values = train_df['proto'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'proto' and calculate the sum for each category\n",
    "proto_sum = train_df.groupby('proto').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "proto_sum.plot(kind='bar')\n",
    "plt.title('Sum of Records by Proto')\n",
    "plt.xlabel('Service')\n",
    "plt.ylabel('Sum of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Service Attributes:\n",
    "- Convert '-' to 0\n",
    "- The rest normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the unique values of the column 'service' in the dataframe 'train_df' \n",
    "unique_values = train_df['service'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'service' and calculate the sum for each category\n",
    "service_sum = train_df.groupby('service').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "service_sum.plot(kind='bar')\n",
    "plt.title('Sum of Records by Service')\n",
    "plt.xlabel('Service')\n",
    "plt.ylabel('Sum of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### State Attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the unique values of the column 'state' in the dataframe 'train_df' \n",
    "unique_values = train_df['state'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'state' and calculate the sum for each category\n",
    "state_sum = train_df.groupby('state').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "state_sum.plot(kind='bar')\n",
    "plt.title('Sum of Records by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Sum of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attack_cat Attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the unique values of the column 'attack_cat' in the dataframe 'train_df' \n",
    "# Target variable: attack_cat\n",
    "unique_values = train_df['attack_cat'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'attack_cat' and calculate the sum for each category\n",
    "attack_cat_sum = train_df.groupby('attack_cat').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "attack_cat_sum.plot(kind='bar')\n",
    "plt.title('Sum of Records by Attack Category')\n",
    "plt.xlabel('Attack Category')\n",
    "plt.ylabel('Sum of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_cat_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'attack_cat': ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Normal', 'Reconnaissance', 'Shellcode', 'Worms'],\n",
    "    'count': [677, 583, 4089, 11132, 6062, 18871, 37000, 3496, 378, 44]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the total count of records\n",
    "total_count = df['count'].sum()\n",
    "\n",
    "# Calculate the percentage for each attribute\n",
    "df['percentage'] = (df['count'] / total_count) * 100\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can consider dropping\n",
    "\n",
    "- tcprtt,Float,\"TCP connection setup round-trip time, the sum of ’synack’ and ’ackdat’.\" -use this is sufficient, 41k '0' values\n",
    "- synack,Float,\"TCP connection setup time, the time between the SYN and the SYN_ACK packets.\" - drop\n",
    "- ackdat,Float,\"TCP connection setup time, the time between the SYN_ACK and the ACK packets.\" -  drop\n",
    "- ct_ftp_cmd,, No of flows that has a command in ftp session. 81652 '0' values - drop\n",
    "- ct_flw_http_mthd, No. of flows that has methods such as Get and Post in http service. 74752 '0' values - drop\n",
    "#### Can consider User-Transformed features\n",
    "- Involved in the creation of the following features:\n",
    "    - srcip: Source IP address - not in \n",
    "    - dstip: Destination IP address - not in\n",
    "    - sport: Source port number - not in\n",
    "    - dsport: Destination port number- not in\n",
    "    - sttl: Source to destination time to live value \n",
    "    - dttl: Destination to source time to live value\n",
    "    - state: \"Indicates to the state and its dependent protocol, e.g. ACC, CLO, CON, ECO, ECR, FIN, INT, MAS, PAR, REQ, RST, TST, TXD, URH, URN, and (-) (if not used state)\"\n",
    "    - service: \"http, ftp, smtp, ssh, dns, ftp-data ,irc  and (-) if not much used service\"\n",
    "    - response_body_len: Actual uncompressed content size of the data transferred from the server’s http service\n",
    "    \n",
    "##### Attributes not in the dataset:\n",
    "- is_sm_ips_ports,Binary,\"If source (srcip) and destination (dstip) IP addresses equal and port numbers (sport)(dsport)  equal then, this variable takes value 1 else 0\"\n",
    "- ct_state_ttl,Integer,No. for each state (state) according to specific range of values for source/destination time to live (sttl) (dttl).\n",
    "##### Attributes still in the dataset:\n",
    "- ct_srv_src,integer,No. of connections that contain the same service (service) and source address (srcip) in 100 connections according to the last time (response_body_len).\n",
    "- ct_srv_dst,integer,No. of connections that contain the same service (service) and destination address (dstip) in 100 connections according to the last time (response_body_len).\n",
    "- ct_dst_ltm,integer,No. of connections of the same destination address (dstip) in 100 connections according to the last time (response_body_len).\n",
    "- ct_src_ ltm,integer,No. of connections of the same source address (srcip) in 100 connections according to the last time (response_body_len).\n",
    "- ct_src_dport_ltm,integer,No of connections of the same source address (srcip) and the destination port (dsport) in 100 connections according to the last time (response_body_len).\n",
    "- ct_dst_sport_ltm,integer,No of connections of the same destination address (dstip) and the source port (sport) in 100 connections according to the last time (response_body_len).\n",
    "- ct_dst_src_ltm,integer,No of connections of the same source (srcip) and the destination (dstip) address in in 100 connections according to the last time (response_body_len).\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tcprtt attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = train_df.groupby('tcprtt').size()\n",
    "unique_values\n",
    "\n",
    "# train_df['tcprtt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### is_sm_ips_ports attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = train_df['is_sm_ips_ports'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "train_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreProcessPipeline(label_encode=True, process_label=True)\n",
    "train_df = pipeline.transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas back to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-----+-----+-----+------+------+-----------+----+----+-------------+-----+-----+-----+---------+------+----+----+----+-----+-----+----+------+------+------+-----+-----+-----------+-----------------+----------+------------+----------+----------------+----------------+--------------+------------+----------+----------------+----------+----------+---------------+-----+\n",
      "|   dur|proto|service|state|spkts|dpkts|sbytes|dbytes|       rate|sttl|dttl|        sload|dload|sloss|dloss|   sinpkt|dinpkt|sjit|djit|swin|stcpb|dtcpb|dwin|tcprtt|synack|ackdat|smean|dmean|trans_depth|response_body_len|ct_srv_src|ct_state_ttl|ct_dst_ltm|ct_src_dport_ltm|ct_dst_sport_ltm|ct_dst_src_ltm|is_ftp_login|ct_ftp_cmd|ct_flw_http_mthd|ct_src_ltm|ct_srv_dst|is_sm_ips_ports|label|\n",
      "+------+-----+-------+-----+-----+-----+------+------+-----------+----+----+-------------+-----+-----+-----+---------+------+----+----+----+-----+-----+----+------+------+------+-----+-----+-----------+-----------------+----------+------------+----------+----------------+----------------+--------------+------------+----------+----------------+----------+----------+---------------+-----+\n",
      "|1.1E-5|    0|      0|    0|    2|    0|   496|     0| 90909.0902| 254|   0| 1.80363632E8|  0.0|    0|    0|    0.011|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  248|    0|          0|                0|         2|           2|         1|               1|               1|             2|           0|         0|               0|         1|         2|              0|    0|\n",
      "|8.0E-6|    0|      0|    0|    2|    0|  1762|     0|125000.0003| 254|   0|       8.81E8|  0.0|    0|    0|    0.008|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  881|    0|          0|                0|         2|           2|         1|               1|               1|             2|           0|         0|               0|         1|         2|              0|    0|\n",
      "|5.0E-6|    0|      0|    0|    2|    0|  1068|     0|200000.0051| 254|   0|      8.544E8|  0.0|    0|    0|    0.005|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  534|    0|          0|                0|         3|           2|         1|               1|               1|             3|           0|         0|               0|         1|         3|              0|    0|\n",
      "|6.0E-6|    0|      0|    0|    2|    0|   900|     0|166666.6608| 254|   0|        6.0E8|  0.0|    0|    0|    0.006|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  450|    0|          0|                0|         3|           2|         2|               2|               1|             3|           0|         0|               0|         2|         3|              0|    0|\n",
      "|1.0E-5|    0|      0|    0|    2|    0|  2126|     0|100000.0025| 254|   0|      8.504E8|  0.0|    0|    0|     0.01|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0| 1063|    0|          0|                0|         3|           2|         2|               2|               1|             3|           0|         0|               0|         2|         3|              0|    0|\n",
      "|3.0E-6|    0|      0|    0|    2|    0|   784|     0|333333.3215| 254|   0|1.045333312E9|  0.0|    0|    0|    0.003|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  392|    0|          0|                0|         2|           2|         2|               2|               1|             2|           0|         0|               0|         2|         2|              0|    0|\n",
      "|6.0E-6|    0|      0|    0|    2|    0|  1960|     0|166666.6608| 254|   0|1.306666624E9|  0.0|    0|    0|    0.006|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  980|    0|          0|                0|         2|           2|         2|               2|               1|             2|           0|         0|               0|         2|         2|              0|    0|\n",
      "|2.8E-5|    0|      0|    0|    2|    0|  1384|     0|35714.28522| 254|   0| 1.97714288E8|  0.0|    0|    0|    0.028|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  692|    0|          0|                0|         3|           2|         1|               1|               1|             3|           0|         0|               0|         1|         3|              0|    0|\n",
      "|   0.0|    1|      0|    0|    1|    0|    46|     0|        0.0|   0|   0|          0.0|  0.0|    0|    0|60000.688|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|   46|    0|          0|                0|         2|           2|         2|               2|               2|             2|           0|         0|               0|         2|         2|              1|    0|\n",
      "|   0.0|    1|      0|    0|    1|    0|    46|     0|        0.0|   0|   0|          0.0|  0.0|    0|    0|60000.712|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|   46|    0|          0|                0|         2|           2|         2|               2|               2|             2|           0|         0|               0|         2|         2|              1|    0|\n",
      "|   0.0|    1|      0|    0|    1|    0|    46|     0|        0.0|   0|   0|          0.0|  0.0|    0|    0|60000.688|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|   46|    0|          0|                0|         2|           2|         2|               2|               2|             2|           0|         0|               0|         2|         2|              1|    0|\n",
      "|   0.0|    1|      0|    0|    1|    0|    46|     0|        0.0|   0|   0|          0.0|  0.0|    0|    0|60000.712|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|   46|    0|          0|                0|         2|           2|         2|               2|               2|             2|           0|         0|               0|         2|         2|              1|    0|\n",
      "|4.0E-6|    0|      0|    0|    2|    0|  1454|     0|250000.0006| 254|   0|      1.454E9|  0.0|    0|    0|    0.004|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  727|    0|          0|                0|         3|           2|         1|               1|               1|             3|           0|         0|               0|         1|         3|              0|    0|\n",
      "|7.0E-6|    0|      0|    0|    2|    0|  2062|     0|142857.1409| 254|   0|1.178285696E9|  0.0|    0|    0|    0.007|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0| 1031|    0|          0|                0|         3|           2|         1|               1|               1|             3|           0|         0|               0|         1|         3|              0|    0|\n",
      "|1.1E-5|    0|      0|    0|    2|    0|  2040|     0| 90909.0902| 254|   0| 7.41818176E8|  0.0|    0|    0|    0.011|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0| 1020|    0|          0|                0|         1|           2|         1|               1|               1|             1|           0|         0|               0|         1|         1|              0|    0|\n",
      "|4.0E-6|    0|      0|    0|    2|    0|  1052|     0|250000.0006| 254|   0|      1.052E9|  0.0|    0|    0|    0.004|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  526|    0|          0|                0|         3|           2|         1|               1|               1|             3|           0|         0|               0|         2|         3|              0|    0|\n",
      "|3.0E-6|    0|      0|    0|    2|    0|   314|     0|333333.3215| 254|   0| 4.18666656E8|  0.0|    0|    0|    0.003|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  157|    0|          0|                0|         2|           2|         1|               1|               1|             2|           0|         0|               0|         1|         2|              0|    0|\n",
      "|1.0E-5|    0|      0|    0|    2|    0|  1774|     0|100000.0025| 254|   0|      7.096E8|  0.0|    0|    0|     0.01|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  887|    0|          0|                0|         2|           2|         1|               1|               1|             2|           0|         0|               0|         1|         2|              0|    0|\n",
      "|2.0E-6|    0|      0|    0|    2|    0|  1568|     0|500000.0013| 254|   0|      3.136E9|  0.0|    0|    0|    0.002|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0|  784|    0|          0|                0|         3|           2|         1|               1|               1|             2|           0|         0|               0|         1|         2|              0|    0|\n",
      "|4.0E-6|    0|      0|    0|    2|    0|  2054|     0|250000.0006| 254|   0|      2.054E9|  0.0|    0|    0|    0.004|   0.0| 0.0| 0.0|   0|    0|    0|   0|   0.0|   0.0|   0.0| 1027|    0|          0|                0|         3|           2|         1|               1|               1|             2|           0|         0|               0|         2|         2|              0|    0|\n",
      "+------+-----+-------+-----+-----+-----+------+------+-----------+----+----+-------------+-----+-----+-----+---------+------+----+----+----+-----+-----+----+------+------+------+-----+-----+-----------+-----------------+----------+------------+----------+----------------+----------------+--------------+------------+----------+----------------+----------+----------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 12:32:59 WARN TaskSetManager: Stage 8 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrames back to Spark DataFrames\n",
    "# train_df_spark = spark.createDataFrame(train_df)\n",
    "# test_df_spark = spark.createDataFrame(test_df_pandas)\n",
    "\n",
    "spark_df = spark.createDataFrame(train_df)\n",
    "spark_df.show()\n",
    "\n",
    "print(type(spark_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training\n",
    "- Select machine learning models (Logistic Regression , Decision Tree, Random Forest, Multilayer perceptron).\n",
    "- Split the data into training and validation sets.\n",
    "- Train the selected models using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input columns (excluding the 'label' column)\n",
    "input_columns = spark_df.columns[:-1]  # Exclude the last column ('label')\n",
    "\n",
    "# Create a VectorAssembler to combine the input columns into a single 'features' column\n",
    "assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "\n",
    "# Transform the DataFrame to add the 'features' column\n",
    "assembled_df = assembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 12:41:10 WARN TaskSetManager: Stage 16 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/11 12:41:12 WARN TaskSetManager: Stage 18 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 20 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 22 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 24 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 26 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 28 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 30 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 32 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 34 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 36 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:14 WARN TaskSetManager: Stage 38 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/11 12:41:15 WARN TaskSetManager: Stage 40 contains a task of very large size (1366 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC =  0.9402999357832275\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_data, val_data = assembled_df.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Build the logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "\n",
    "# Create a pipeline for the model\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = model.transform(val_data)\n",
    "\n",
    "# Evaluate the model using a BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"AUC = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comparison between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI316",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
