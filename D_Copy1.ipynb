{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Assignment 2 \n",
    "\n",
    "### Authors: \n",
    "-  Chin Yee Wan \n",
    "-  Darrel Koh\n",
    "-  Nguyen Gia Khanh \n",
    "-  Ngo Vu Anh\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and Visualise the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                            .appName(\"CSCI316GP2\")\\\n",
    "                            .config(\"spark.sql.files.maxPartitionBytes\", \"1000000\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(train_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom pipeline for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = True):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label:\n",
    "            def label_transformer(category):\n",
    "                if category == 'Normal':\n",
    "                    return 0\n",
    "                elif category in ['Reconnaissance', 'Analysis', 'Fuzzers', 'Shellcode', 'Generic']:\n",
    "                    return 0\n",
    "                elif category in ['Backdoor', 'DoS', 'Exploits', 'Worms']:\n",
    "                    return 1\n",
    "\n",
    "            df['label'] = df['attack_cat'].apply(label_transformer)\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_data):\n",
    "    # Make predictions on the validation data\n",
    "    predictions = model.transform(val_data)\n",
    "\n",
    "    # Evaluate the model using a BinaryClassificationEvaluator for AUC\n",
    "    auc_evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Evaluate the model using a BinaryClassificationEvaluator for areaUnderPR\n",
    "    pr_evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderPR')\n",
    "    area_under_pr = pr_evaluator.evaluate(predictions)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    sensitivity = predictions.filter(\"label = 1 and prediction = 1\").count() / predictions.filter(\"label = 1\").count()\n",
    "    specificity = predictions.filter(\"label = 0 and prediction = 0\").count() / predictions.filter(\"label = 0\").count()\n",
    "    precision = predictions.filter(\"prediction = 1\").count() / predictions.filter(\"prediction = 1 or prediction = 0\").count()\n",
    "    recall = sensitivity\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (predictions.filter(\"label = prediction\").count()) / predictions.count()\n",
    "    # youdens_j = sensitivity + specificity - 1\n",
    "    # balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "    # Create a dictionary of model evaluation metrics\n",
    "    eval_metrics = {\n",
    "        'AUC': auc,\n",
    "        'AreaUnderPR': area_under_pr,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1_score,\n",
    "        'Accuracy': accuracy,\n",
    "        # \"Youden's J Index\": youdens_j,\n",
    "        # 'Balanced Accuracy': balanced_accuracy\n",
    "    }\n",
    "\n",
    "    return eval_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Spark to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrames to Pandas\n",
    "# train_df = train_df.toPandas()\n",
    "# test_df_pandas = test_df.toPandas()\n",
    "# features_df_pandas = features_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Features set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df.head(20)\n",
    "# features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "# train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of zeros in each column:\n",
    "zero_counts = {}\n",
    "\n",
    "for column in train_df.columns:\n",
    "    zero_counts[column] = (train_df[column] == \"0\").sum()\n",
    "    \n",
    "# Create a DataFrame from the zero_counts dictionary\n",
    "zero_counts_df = pd.DataFrame(list(zero_counts.items()), columns=['Column', 'Zero Count'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(zero_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 Objects that will require Encoding\n",
    "- proto \n",
    "- service\n",
    "- state\n",
    "- attack_cat (1 of target variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proto Attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the unique values of the column 'proto' in the dataframe 'train_df' \n",
    "unique_values = train_df['proto'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'proto' and calculate the sum for each category\n",
    "proto_sum = train_df.groupby('proto').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "proto_sum.plot(kind='bar')\n",
    "plt.title('Sum of Records by Proto')\n",
    "plt.xlabel('Service')\n",
    "plt.ylabel('Sum of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Service Attributes:\n",
    "- Convert '-' to 0\n",
    "- The rest normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the unique values of the column 'service' in the dataframe 'train_df' \n",
    "unique_values = train_df['service'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'service' and calculate the sum for each category\n",
    "service_sum = train_df.groupby('service').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "service_sum.plot(kind='bar')\n",
    "plt.title('Sum of Records by Service')\n",
    "plt.xlabel('Service')\n",
    "plt.ylabel('Sum of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### State Attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the unique values of the column 'state' in the dataframe 'train_df' \n",
    "unique_values = train_df['state'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'state' and calculate the sum for each category\n",
    "state_sum = train_df.groupby('state').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "state_sum.plot(kind='bar')\n",
    "plt.title('Sum of Records by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Sum of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attack_cat Attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the unique values of the column 'attack_cat' in the dataframe 'train_df' \n",
    "# Target variable: attack_cat\n",
    "unique_values = train_df['attack_cat'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'attack_cat' and calculate the sum for each category\n",
    "attack_cat_sum = train_df.groupby('attack_cat').size()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "attack_cat_sum.plot(kind='bar')\n",
    "plt.title('Sum of Records by Attack Category')\n",
    "plt.xlabel('Attack Category')\n",
    "plt.ylabel('Sum of Records')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_cat_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'attack_cat': ['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic', 'Normal', 'Reconnaissance', 'Shellcode', 'Worms'],\n",
    "    'count': [677, 583, 4089, 11132, 6062, 18871, 37000, 3496, 378, 44]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the total count of records\n",
    "total_count = df['count'].sum()\n",
    "\n",
    "# Calculate the percentage for each attribute\n",
    "df['percentage'] = (df['count'] / total_count) * 100\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can consider dropping\n",
    "\n",
    "- tcprtt,Float,\"TCP connection setup round-trip time, the sum of ’synack’ and ’ackdat’.\" -use this is sufficient, 41k '0' values\n",
    "- synack,Float,\"TCP connection setup time, the time between the SYN and the SYN_ACK packets.\" - drop\n",
    "- ackdat,Float,\"TCP connection setup time, the time between the SYN_ACK and the ACK packets.\" -  drop\n",
    "- ct_ftp_cmd,, No of flows that has a command in ftp session. 81652 '0' values - drop\n",
    "- ct_flw_http_mthd, No. of flows that has methods such as Get and Post in http service. 74752 '0' values - drop\n",
    "#### Can consider User-Transformed features\n",
    "- Involved in the creation of the following features:\n",
    "    - srcip: Source IP address - not in \n",
    "    - dstip: Destination IP address - not in\n",
    "    - sport: Source port number - not in\n",
    "    - dsport: Destination port number- not in\n",
    "    - sttl: Source to destination time to live value \n",
    "    - dttl: Destination to source time to live value\n",
    "    - state: \"Indicates to the state and its dependent protocol, e.g. ACC, CLO, CON, ECO, ECR, FIN, INT, MAS, PAR, REQ, RST, TST, TXD, URH, URN, and (-) (if not used state)\"\n",
    "    - service: \"http, ftp, smtp, ssh, dns, ftp-data ,irc  and (-) if not much used service\"\n",
    "    - response_body_len: Actual uncompressed content size of the data transferred from the server’s http service\n",
    "    \n",
    "##### Attributes not in the dataset:\n",
    "- is_sm_ips_ports,Binary,\"If source (srcip) and destination (dstip) IP addresses equal and port numbers (sport)(dsport)  equal then, this variable takes value 1 else 0\"\n",
    "- ct_state_ttl,Integer,No. for each state (state) according to specific range of values for source/destination time to live (sttl) (dttl).\n",
    "##### Attributes still in the dataset:\n",
    "- ct_srv_src,integer,No. of connections that contain the same service (service) and source address (srcip) in 100 connections according to the last time (response_body_len).\n",
    "- ct_srv_dst,integer,No. of connections that contain the same service (service) and destination address (dstip) in 100 connections according to the last time (response_body_len).\n",
    "- ct_dst_ltm,integer,No. of connections of the same destination address (dstip) in 100 connections according to the last time (response_body_len).\n",
    "- ct_src_ ltm,integer,No. of connections of the same source address (srcip) in 100 connections according to the last time (response_body_len).\n",
    "- ct_src_dport_ltm,integer,No of connections of the same source address (srcip) and the destination port (dsport) in 100 connections according to the last time (response_body_len).\n",
    "- ct_dst_sport_ltm,integer,No of connections of the same destination address (dstip) and the source port (sport) in 100 connections according to the last time (response_body_len).\n",
    "- ct_dst_src_ltm,integer,No of connections of the same source (srcip) and the destination (dstip) address in in 100 connections according to the last time (response_body_len).\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tcprtt attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = train_df.groupby('tcprtt').size()\n",
    "unique_values\n",
    "\n",
    "# train_df['tcprtt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### is_sm_ips_ports attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = train_df['is_sm_ips_ports'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "train_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreProcessPipeline(label_encode=True, process_label=True)\n",
    "train_df = pipeline.transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas back to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames back to Spark DataFrames\n",
    "# train_df_spark = spark.createDataFrame(train_df)\n",
    "# test_df_spark = spark.createDataFrame(test_df_pandas)\n",
    "\n",
    "spark_df = spark.createDataFrame(train_df)\n",
    "spark_df.show()\n",
    "\n",
    "print(type(spark_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble columns into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition the DataFrame into a desired number of partitions\n",
    "num_partitions = 500\n",
    "repartitioned_df = spark_df.repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input columns (excluding the 'label' column)\n",
    "input_columns = spark_df.columns[:-1]  # Exclude the last column ('label')\n",
    "\n",
    "# Create a VectorAssembler to combine the input columns into a single 'features' column\n",
    "assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "\n",
    "\n",
    "# Transform the DataFrame to add the 'features' column\n",
    "assembled_df = assembler.transform(repartitioned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_data, val_data = assembled_df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.select('features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training\n",
    "- Select machine learning models (Logistic Regression , Decision Tree, Random Forest, Multilayer perceptron).\n",
    "- Split the data into training and validation sets.\n",
    "- Train the selected models using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for default and fine-tuned models\n",
    "lr_default = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "pipeline_default = Pipeline(stages=[lr_default])\n",
    "model_default = pipeline_default.fit(train_data)\n",
    "\n",
    "# Evaluate models using the evaluate_model function\n",
    "evaluation_results_default = evaluate_model(model_default, val_data)\n",
    "\n",
    "# Print evaluation results for default model\n",
    "print(\"Default Model Evaluation:\")\n",
    "for metric, value in evaluation_results_default.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for default and fine-tuned models\n",
    "lr_default = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "pipeline_default = Pipeline(stages=[lr_default])\n",
    "model_default = pipeline_default.fit(assembled_df)\n",
    "\n",
    "\n",
    "# # Build the logistic regression model\n",
    "# lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "\n",
    "# # Create a pipeline for the model\n",
    "# pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# model = pipeline.fit(train_data)\n",
    "\n",
    "# # Make predictions on the validation data\n",
    "# predictions = model.transform(val_data)\n",
    "\n",
    "# # Evaluate the model using a BinaryClassificationEvaluator\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "# auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# print(\"AUC = \", auc)\n",
    "\n",
    "# print(\"Default Model Evaluation:\")\n",
    "# for metric, value in evaluation_results_default.items():\n",
    "#     print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comparison between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI316",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
