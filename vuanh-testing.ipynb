{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline for dat pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = 'Binary'):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state', 'attack_cat']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label == 'Binary':\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "        else:             \n",
    "            df['attack_cat'], df['label'] = df['label'], df['attack_cat']\n",
    "            print('change name')\n",
    "            df.drop('attack_cat', axis=1, inplace=True)      \n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "dur                  0\n",
       "proto                0\n",
       "service              0\n",
       "state                0\n",
       "spkts                0\n",
       "dpkts                0\n",
       "sbytes               0\n",
       "dbytes               0\n",
       "rate                 0\n",
       "sttl                 0\n",
       "dttl                 0\n",
       "sload                0\n",
       "dload                0\n",
       "sloss                0\n",
       "dloss                0\n",
       "sinpkt               0\n",
       "dinpkt               0\n",
       "sjit                 0\n",
       "djit                 0\n",
       "swin                 0\n",
       "stcpb                0\n",
       "dtcpb                0\n",
       "dwin                 0\n",
       "tcprtt               0\n",
       "synack               0\n",
       "ackdat               0\n",
       "smean                0\n",
       "dmean                0\n",
       "trans_depth          0\n",
       "response_body_len    0\n",
       "ct_srv_src           0\n",
       "ct_state_ttl         0\n",
       "ct_dst_ltm           0\n",
       "ct_src_dport_ltm     0\n",
       "ct_dst_sport_ltm     0\n",
       "ct_dst_src_ltm       0\n",
       "is_ftp_login         0\n",
       "ct_ftp_cmd           0\n",
       "ct_flw_http_mthd     0\n",
       "ct_src_ltm           0\n",
       "ct_srv_dst           0\n",
       "is_sm_ips_ports      0\n",
       "attack_cat           0\n",
       "label                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreProcessPipeline(label_encode=True, process_label=\"Binary\")\n",
    "train_df = pipeline.transform(train_df)\n",
    "test_df = pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "train_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/14 10:15:31 WARN Utils: Your hostname, Vus-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.33.27.219 instead (on interface en0)\n",
      "23/08/14 10:15:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/14 10:15:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316GP2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparktrain_df = spark.createDataFrame(train_df)\n",
    "sparktest_df = spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Khanh Nguyen\n",
    "Name: PySpark Dataframe Pipeline\n",
    "Description:\n",
    "    This class is used to create a pipeline for PySpark dataframe, accept 2 boolean parameter: smote & standardize.\n",
    "    Features \n",
    "        (Default)\n",
    "        - Resample: Resample the dataframe\n",
    "        - Vectorize: Vectorize the dataframe\n",
    "        (activate by setting the parameter to True):\n",
    "        - SMOTE: Oversampling the minority class\n",
    "        - Standardize: Standardize the dataframe using z-score\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "class SparkDFPipeline:\n",
    "    def __init__(self, smote=False, standardize=False):\n",
    "        self.smote = smote\n",
    "        self.standardize = standardize\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, train_df, test_df):\n",
    "        if self.smote:\n",
    "            majority = train_df.filter(col('label') == 0)\n",
    "            minority = train_df.filter(col('label') == 1)\n",
    "\n",
    "            majority_count = majority.count()\n",
    "            minority_count = minority.count()\n",
    "\n",
    "            ratio = int(majority_count / minority_count)\n",
    "            sample_num = int(ratio * minority_count) - minority_count\n",
    "            sample = minority.sample(True, sample_num / minority_count, seed=42)\n",
    "            balanced_sample = minority.union(sample)\n",
    "            train_df = majority.union(balanced_sample).orderBy('label')\n",
    "        \n",
    "        if self.standardize:\n",
    "            # Standardize the df\n",
    "\n",
    "            # Resample the df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "\n",
    "            exclude = ['proto', 'service', 'state']\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            selected_columns = [col for col in input_columns if col not in exclude]\n",
    "            # Vectorize the df\n",
    "            assembler = VectorAssembler(inputCols=selected_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            test_df = assembler.transform(test_df)\n",
    "\n",
    "            # Standardize the df\n",
    "            scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "            scaler_model = scaler.fit(train_df)\n",
    "            train_df = scaler_model.transform(train_df)\n",
    "\n",
    "            scaler_model = scaler.fit(test_df)\n",
    "            test_df = scaler_model.transform(test_df)\n",
    "            test_df = test_df.drop('features')\n",
    "            train_df = train_df.drop('features')\n",
    "            \n",
    "            # put back the categorical columns\n",
    "            input_cols = ['scaled_features', 'proto', 'service', 'state']\n",
    "            output_col = \"features\"\n",
    "            assembler1 = VectorAssembler(inputCols=input_cols, outputCol=output_col)\n",
    "            train_df = assembler1.transform(train_df)\n",
    "            test_df = assembler1.transform(test_df)\n",
    "\n",
    "            # return result\n",
    "            test_df = test_df.select('features', 'label')\n",
    "            train_df = train_df.select('features', 'label')\n",
    "        else:\n",
    "            # Normal vectorize df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            test_df = assembler.transform(test_df)\n",
    "            train_df = train_df.select('features', 'label')\n",
    "              \n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/14 10:16:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/08/14 10:16:16 WARN TaskSetManager: Stage 0 contains a task of very large size (1715 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/14 10:16:27 WARN TaskSetManager: Stage 6 contains a task of very large size (3665 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pipeline example\n",
    "pipeline = SparkDFPipeline(smote=False, standardize=True)\n",
    "train, test = pipeline.transform(sparktrain_df, sparktest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/14 10:16:34 WARN TaskSetManager: Stage 9 contains a task of very large size (1715 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/14 10:16:35 WARN TaskSetManager: Stage 10 contains a task of very large size (1715 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/14 10:16:43 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/14 10:20:11 WARN TaskSetManager: Stage 701 contains a task of very large size (1715 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/14 10:20:13 WARN TaskSetManager: Stage 702 contains a task of very large size (1715 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 702:>                                                        (0 + 8) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Train an SVM model\n",
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100)  # Adding aggregationDepth for performance\n",
    "svm_model = svm.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/14 10:20:20 WARN TaskSetManager: Stage 703 contains a task of very large size (3665 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8457748045237565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/14 10:20:22 WARN TaskSetManager: Stage 705 contains a task of very large size (3665 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.82413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/14 10:20:24 WARN TaskSetManager: Stage 716 contains a task of very large size (3665 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.876701\n"
     ]
    }
   ],
   "source": [
    "# Make prediction \n",
    "\n",
    "predictions = svm_model.transform(test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "roc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "roc_score = roc_evaluator.evaluate(predictions)\n",
    "print(\"Area under ROC = %g\" % roc_score)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "pr_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
    "pr_score = pr_evaluator.evaluate(predictions)\n",
    "print(\"Area under PR = %g\" % pr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.tol, [1e-5, 1e-6, 1e-7]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator for model selection\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Create CrossValidator\n",
    "cross_val = CrossValidator(estimator=svm,\n",
    "                           estimatorParamMaps=param_grid,\n",
    "                           evaluator=evaluator,\n",
    "                           numFolds=5)  # Number of folds for cross-validation\n",
    "\n",
    "# Fit CrossValidator on train data\n",
    "cv_model = cross_val.fit(train)\n",
    "\n",
    "# Get the best LinearSVC model from the cross-validation\n",
    "best_svm_model = cv_model.bestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction \n",
    "\n",
    "predictions = best_svm_model.transform(test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "roc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "roc_score = roc_evaluator.evaluate(predictions)\n",
    "print(\"Area under ROC = %g\" % roc_score)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "pr_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
    "pr_score = pr_evaluator.evaluate(predictions)\n",
    "print(\"Area under PR = %g\" % pr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
