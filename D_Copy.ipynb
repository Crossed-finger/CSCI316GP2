{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 4 Assignment 2 \n",
    "\n",
    "### Authors: \n",
    "-  Chin Yee Wan \n",
    "-  Darrel Koh\n",
    "-  Nguyen Gia Khanh \n",
    "-  Ngo Vu Anh\t\n",
    "\n",
    "### Main Steps\n",
    "1.  Data Preprocessing \n",
    "-   Read in as SPARK dataframe for data preprocessing\n",
    "-   Convert to Pandas dataframe for data exploration\n",
    "2.  Data Exploration\n",
    "3.  Data Modelling\n",
    "4.  Data Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and Visualise the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                            .appName(\"CSCI316GP2\")\\\n",
    "                            .config(\"spark.sql.files.maxPartitionBytes\", \"1000000\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(train_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom pipeline for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = 'Binary'):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state', 'attack_cat']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label == 'Binary':\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "        else:             \n",
    "            df['attack_cat'], df['label'] = df['label'], df['attack_cat']\n",
    "            print('change name')\n",
    "            df.drop('attack_cat', axis=1, inplace=True)      \n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Khanh Nguyen\n",
    "Name: PySpark Dataframe Pipeline\n",
    "Description:\n",
    "    This class is used to create a pipeline for PySpark dataframe, accept 2 boolean parameter: smote & standardize.\n",
    "    Features \n",
    "        (Default)\n",
    "        - Resample: Resample the dataframe\n",
    "        - Vectorize: Vectorize the dataframe\n",
    "        (activate by setting the parameter to True):\n",
    "        - SMOTE: Oversampling the minority class\n",
    "        - Standardize: Standardize the dataframe using z-score\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "class SparkDFPipeline:\n",
    "    def __init__(self, smote=False, standardize=False):\n",
    "        self.smote = smote\n",
    "        self.standardize = standardize\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, train_df, test_df):\n",
    "        if self.smote:\n",
    "            majority = train_df.filter(col('label') == 0)\n",
    "            minority = train_df.filter(col('label') == 1)\n",
    "\n",
    "            majority_count = majority.count()\n",
    "            minority_count = minority.count()\n",
    "\n",
    "            ratio = int(majority_count / minority_count)\n",
    "            sample_num = int(ratio * minority_count) - minority_count\n",
    "            sample = minority.sample(True, sample_num / minority_count, seed=42)\n",
    "            balanced_sample = minority.union(sample)\n",
    "            train_df = majority.union(balanced_sample).orderBy('label')\n",
    "        \n",
    "        if self.standardize:\n",
    "            # Standardize the df\n",
    "\n",
    "            # Resample the df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "\n",
    "            exclude = ['proto', 'service', 'state']\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            selected_columns = [col for col in input_columns if col not in exclude]\n",
    "            # Vectorize the df\n",
    "            assembler = VectorAssembler(inputCols=selected_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            test_df = assembler.transform(test_df)\n",
    "\n",
    "            # Standardize the df\n",
    "            scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "            scaler_model = scaler.fit(train_df)\n",
    "            train_df = scaler_model.transform(train_df)\n",
    "\n",
    "            scaler_model = scaler.fit(test_df)\n",
    "            test_df = scaler_model.transform(test_df)\n",
    "            test_df = test_df.drop('features')\n",
    "            train_df = train_df.drop('features')\n",
    "            \n",
    "            # put back the categorical columns\n",
    "            input_cols = ['scaled_features', 'proto', 'service', 'state']\n",
    "            output_col = \"features\"\n",
    "            assembler1 = VectorAssembler(inputCols=input_cols, outputCol=output_col)\n",
    "            train_df = assembler1.transform(train_df)\n",
    "            test_df = assembler1.transform(test_df)\n",
    "\n",
    "            # return result\n",
    "            test_df = test_df.select('features', 'label')\n",
    "            train_df = train_df.select('features', 'label')\n",
    "        else:\n",
    "            # Normal vectorize df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            train_df = train_df.select('features', 'label')\n",
    "            test_df = assembler.transform(test_df)\n",
    "              \n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your class labels\n",
    "class_labels = ['Normal', 'Generic', 'Exploits', 'Fuzzers', 'DoS', 'Reconnaissance', 'Analysis', 'Backdoor', 'Shellcode', 'Worms']\n",
    "\n",
    "def evaluate_model(model, val_data, model_name):\n",
    "    # Make predictions on the validation data\n",
    "    predictions = model.transform(val_data)\n",
    "\n",
    "    acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = acc_evaluator.evaluate(predictions)\n",
    "\n",
    "    f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1_score = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "    # AUC_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "    # AUC_score = AUC_evaluator.evaluate(predictions)\n",
    "\n",
    "    # AUPR_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
    "    # AUPR_score = AUPR_evaluator.evaluate(predictions)\n",
    "\n",
    "    precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "    precision_score = precision_evaluator.evaluate(predictions)\n",
    "\n",
    "    recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    recall_score = recall_evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Convert Spark DataFrames to Pandas DataFrames for visualization\n",
    "    y_true_pd = predictions.select('label').toPandas()\n",
    "    y_pred_pd = predictions.select('prediction').toPandas()\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_true_pd['label'], y_pred_pd['prediction'])\n",
    "\n",
    "    # Get the predicted counts for each class label\n",
    "    predicted_counts = y_pred_pd['prediction'].value_counts()\n",
    "\n",
    "    # Create a dictionary to store the counts of each class label\n",
    "    class_counts = {label: 0 for label in class_labels}\n",
    "\n",
    "    # Fill in the dictionary with actual predicted counts where available\n",
    "    for key, count in predicted_counts.items():\n",
    "        class_counts[class_labels[int(key)]] = count\n",
    "\n",
    "    # Convert the dictionary values to a list\n",
    "    predicted_counts_list = [class_counts[label] for label in class_labels]\n",
    "\n",
    "    # Display the confusion matrix as a heatmap with sorted class labels and counts\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels[::-1], yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Create a dictionary of model evaluation metrics\n",
    "    eval_metrics = {\n",
    "        'Accuracy': accuracy\n",
    "        , 'F1 Score': f1_score\n",
    "        # , 'Area Under ROC': AUC_score\n",
    "        # , 'Area Under PR': AUPR_score\n",
    "        , 'Precision': precision_score\n",
    "        , 'Recall': recall_score\n",
    "    }\n",
    "\n",
    "    return eval_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore train_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreProcessPipeline(label_encode=True, process_label='Multi')\n",
    "train_df = pipeline.transform(train_df)\n",
    "test_df = pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"figure.figsize\"]=(20,22)\n",
    "train_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas DF to Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparktrain_df = spark.createDataFrame(train_df)\n",
    "sparktest_df = spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature enabler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SparkDFPipeline(smote=False, standardize=False)\n",
    "train, test = pipeline.transform(sparktrain_df, sparktest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SparkDFPipeline(smote=False, standardize=True)\n",
    "FT_train, FT_test = pipeline.transform(sparktrain_df, sparktest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training\n",
    "- Select machine learning models (Logistic Regression , Decision Tree, Random Forest, Multilayer perceptron).\n",
    "- Split the data into training and validation sets.\n",
    "- Train the selected models using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.01, elasticNetParam=0.8)\n",
    "\n",
    "# Train the model\n",
    "model = lr.fit(train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage for default model\n",
    "lr_default = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "pipeline_default = Pipeline(stages=[lr_default])\n",
    "model_default = pipeline_default.fit(train)  # Use the 'train' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage for fine-tuned models\n",
    "lr_tuned = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "pipeline_tuned = Pipeline(stages=[lr_tuned])\n",
    "model_tuned = pipeline_tuned.fit(FT_train)  # Use the 'FT_train' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stages for your pipeline\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "\n",
    "# Create a pipeline with the defined stages\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Define the ParamGrid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter, [10, 20, 30]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# Instantiate CrossValidator with the pipeline and paramGrid\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Fit the CrossValidator on your training data\n",
    "best_tuned_model = cv.fit(FT_train).bestModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models using the evaluate_model_multiclass function\n",
    "evaluation_results_default = evaluate_model(model_default, test, 'Default Model')  # Use the 'test' dataset\n",
    "\n",
    "# Evaluate models using the evaluate_model_multiclass function\n",
    "evaluation_results_tuned = evaluate_model(model_tuned, FT_test, 'Fine-Tuned Model')  # Use the 'FT_test' dataset\n",
    "\n",
    "# Corrected variable name\n",
    "evaluation_results_best_tuned = evaluate_model(best_tuned_model, FT_test, 'Best-Tuned Model')  # Use the 'FT_test' dataset\n",
    "\n",
    "# Print evaluation results for all models side by side\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"{'Metric':<20}{'Default Model':<20}{'Fine-Tuned Model':<20}{'Best-Tuned Model':<20}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric in evaluation_results_default.keys():\n",
    "    default_value = evaluation_results_default[metric]\n",
    "    tuned_value = evaluation_results_tuned[metric]\n",
    "    best_tuned_value = evaluation_results_best_tuned[metric]\n",
    "    print(f\"{metric:<20}{default_value:<20.6f}{tuned_value:<20.6f}{best_tuned_value:<20.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sci-kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# def evaluate_model_with_visualization(model, val_data, model_name):\n",
    "#     # Make predictions on the validation data\n",
    "    # predictions = model.transform(val_data)\n",
    "\n",
    "#     # Evaluate the model using a BinaryClassificationEvaluator for AUC\n",
    "#     auc_evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "#     auc = auc_evaluator.evaluate(predictions)\n",
    "\n",
    "#     # Calculate additional metrics\n",
    "#     sensitivity = predictions.filter(\"label = 1 and prediction = 1\").count() / predictions.filter(\"label = 1\").count()\n",
    "#     specificity = predictions.filter(\"label = 0 and prediction = 0\").count() / predictions.filter(\"label = 0\").count()\n",
    "#     precision = predictions.filter(\"prediction = 1\").count() / predictions.filter(\"prediction = 1 or prediction = 0\").count()\n",
    "#     recall = sensitivity\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#     accuracy = (predictions.filter(\"label = prediction\").count()) / predictions.count()\n",
    "\n",
    "#     # Convert Spark DataFrames to Pandas DataFrames for visualization\n",
    "#     y_true_pd = predictions.select('label').toPandas()\n",
    "#     y_pred_pd = predictions.select('prediction', 'probability').toPandas()\n",
    "    \n",
    "#     # Convert prediction probabilities to binary predictions\n",
    "#     y_pred_binary = [1 if prob[1] >= 0.5 else 0 for prob in y_pred_pd['probability']]\n",
    "\n",
    "#     # Generate the confusion matrix\n",
    "#     cm = confusion_matrix(y_true_pd['label'], y_pred_binary)\n",
    "\n",
    "#     # Display the confusion matrix as a heatmap\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-default', 'Default'], \n",
    "#                 yticklabels=['Non-default', 'Default'])\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.ylabel('True')\n",
    "#     plt.title(model_name)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Generate the classification report\n",
    "#     report = classification_report(y_true_pd['label'], y_pred_binary, target_names=['Non-default', 'Default'], output_dict=True)\n",
    "\n",
    "#     # Create a summary table\n",
    "#     summary_table = pd.DataFrame({\n",
    "#         'Model': [model_name],\n",
    "#         'AUC': [auc],\n",
    "#         'Sensitivity': [sensitivity],\n",
    "#         'Specificity': [specificity],\n",
    "#         'Precision': [precision],\n",
    "#         'Recall': [recall],\n",
    "#         'F1-Score': [f1_score],\n",
    "#         'Accuracy': [accuracy],\n",
    "#         'Precision (Non-default)': [report['Non-default']['precision']],\n",
    "#         'Recall (Non-default)': [report['Non-default']['recall']],\n",
    "#         'F1-score (Non-default)': [report['Non-default']['f1-score']],\n",
    "#         'Precision (Default)': [report['Default']['precision']],\n",
    "#         'Recall (Default)': [report['Default']['recall']],\n",
    "#         'F1-score (Default)': [report['Default']['f1-score']],\n",
    "#     })\n",
    "\n",
    "#     return summary_table\n",
    "\n",
    "# # Assuming you have 'model_default' and 'test' DataFrame from your pipeline\n",
    "# evaluation_results_default = evaluate_model_with_visualization(model_default, test, 'Default Model')\n",
    "# print(evaluation_results_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # Assuming you have the true labels and predicted labels for your test data\n",
    "# true_labels = test.select('label').rdd.flatMap(lambda x: x).collect()\n",
    "# predicted_labels = model_default.transform(test).select('prediction').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# # Calculate the confusion matrix\n",
    "# cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# # Print the confusion matrix\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comparison between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
