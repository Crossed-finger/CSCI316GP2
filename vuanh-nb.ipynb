{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('GA2Datasets/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('GA2Datasets/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline for dat pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "class PreProcessPipeline:\n",
    "    def __init__(self, label_encode = True, process_label = 'Binary', smote = False):\n",
    "        self.label_encode = label_encode\n",
    "        self.process_label = process_label\n",
    "        self.smote = smote\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop('id', axis=1)\n",
    "        df = df.copy()\n",
    "        if self.label_encode:\n",
    "            columns = ['proto', 'service', 'state', 'attack_cat']\n",
    "            for column in columns:\n",
    "                unique_values = df[column].unique()\n",
    "                mapping = {value: index for index, value in enumerate(unique_values)}\n",
    "                df[column] = df[column].map(mapping)\n",
    "\n",
    "        if self.process_label == 'Binary':\n",
    "            df.drop('attack_cat', axis=1, inplace=True)\n",
    "        else:             \n",
    "            df['attack_cat'], df['label'] = df['label'], df['attack_cat']\n",
    "            df.drop('attack_cat', axis=1, inplace=True)   \n",
    "\n",
    "        if self.smote:\n",
    "            # Separate features and labels\n",
    "            X = df.drop('label', axis=1)\n",
    "            y = df['label']\n",
    "\n",
    "            # Apply SMOTE for oversampling\n",
    "            smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "            # Convert NumPy arrays back to Pandas DataFrames\n",
    "            X_resampled_df = pd.DataFrame(data=X_resampled, columns=X.columns)\n",
    "            y_resampled_df = pd.DataFrame(data=y_resampled, columns=['label'])\n",
    "\n",
    "            # Concatenate the features and label columns into a single DataFrame\n",
    "            df = pd.concat([X_resampled_df, y_resampled_df], axis=1)   \n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_train = PreProcessPipeline(label_encode=True, process_label='Binary')\n",
    "pipeline_test = PreProcessPipeline(label_encode=True, process_label='Binary')\n",
    "train_df = pipeline_train.transform(train_df)\n",
    "test_df = pipeline_test.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plot countplot for column1\n",
    "sns.countplot(data=train_df, x='label', palette=\"Set1\", ax=axes[0])\n",
    "axes[0].set_title(\"Label Plot for train df\")\n",
    "axes[0].set_xlabel(\"Column 1\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot countplot for column2\n",
    "sns.countplot(data=test_df, x='label', palette=\"Set2\", ax=axes[1])\n",
    "axes[1].set_title(\"Label Plot for test df\")\n",
    "axes[1].set_xlabel(\"Column 2\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.hist()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - Correlation matrix\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = train_df.corr()\n",
    "\n",
    "# Select the correlation values with 'label', label here means attack_cat\n",
    "target_corr = corr_matrix['label']\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature ranking in corr order \n",
    "\n",
    "# Calculate the absolute correlation values with the 'label'\n",
    "target_corr_abs = corr_matrix['label'].abs()\n",
    "\n",
    "# Sort the correlation values in descending order\n",
    "sorted_corr = target_corr_abs.sort_values(ascending=False)\n",
    "\n",
    "# Print the sorted correlation values and their corresponding attributes\n",
    "for attribute, correlation in sorted_corr.items():\n",
    "    print(f\"{attribute}: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316GP2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparktrain_df = spark.createDataFrame(train_df)\n",
    "sparktest_df = spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Khanh Nguyen\n",
    "Name: PySpark Dataframe Pipeline\n",
    "Description:\n",
    "    This class is used to create a pipeline for PySpark dataframe, accept 2 boolean parameter: smote & standardize.\n",
    "    Features \n",
    "        (Default)\n",
    "        - Resample: Resample the dataframe\n",
    "        - Vectorize: Vectorize the dataframe\n",
    "        (activate by setting the parameter to True):\n",
    "        - SMOTE: Oversampling the minority class\n",
    "        - Standardize: Standardize the dataframe using z-score\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "class SparkDFPipeline:\n",
    "    def __init__(self, standardize=False):\n",
    "        self.standardize = standardize\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, train_df, test_df):      \n",
    "        if self.standardize:\n",
    "            # Standardize the df\n",
    "\n",
    "            # Resample the df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "\n",
    "            exclude = ['proto', 'service', 'state']\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            selected_columns = [col for col in input_columns if col not in exclude]\n",
    "            # Vectorize the df\n",
    "            assembler = VectorAssembler(inputCols=selected_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            test_df = assembler.transform(test_df)\n",
    "\n",
    "            # Standardize the df\n",
    "            scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "            scaler_model = scaler.fit(train_df)\n",
    "            train_df = scaler_model.transform(train_df)\n",
    "\n",
    "            scaler_model = scaler.fit(test_df)\n",
    "            test_df = scaler_model.transform(test_df)\n",
    "            test_df = test_df.drop('features')\n",
    "            train_df = train_df.drop('features')\n",
    "            \n",
    "            # put back the categorical columns\n",
    "            input_cols = ['scaled_features', 'proto', 'service', 'state']\n",
    "            output_col = \"features\"\n",
    "            assembler1 = VectorAssembler(inputCols=input_cols, outputCol=output_col)\n",
    "            train_df = assembler1.transform(train_df)\n",
    "            test_df = assembler1.transform(test_df)\n",
    "\n",
    "            # return result\n",
    "            test_df = test_df.select('features', 'label')\n",
    "            train_df = train_df.select('features', 'label')\n",
    "        else:\n",
    "            # Normal vectorize df\n",
    "            num_partitions = 500\n",
    "            repartitioned_df = train_df.repartition(num_partitions)\n",
    "            input_columns = train_df.columns[:-1]\n",
    "            assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "            train_df = assembler.transform(repartitioned_df)\n",
    "            train_df = train_df.select('features', 'label')\n",
    "            test_df = assembler.transform(test_df)\n",
    "              \n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline example\n",
    "pipeline = SparkDFPipeline(smote=False, standardize=False)\n",
    "train, test = pipeline.transform(sparktrain_df, sparktest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the weight for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class frequencies\n",
    "class_frequencies = train.groupBy(\"label\").count()\n",
    "\n",
    "# Calculate class weights\n",
    "total_samples = train.count()\n",
    "class_frequencies = class_frequencies.withColumn(\"weight\", total_samples / (class_frequencies[\"count\"] * class_frequencies.count()))\n",
    "\n",
    "# Join the weights with the training data\n",
    "train_with_weights = train.join(class_frequencies, on=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")\n",
    "nb_model = nb.fit(train_with_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction \n",
    "\n",
    "predictions = nb_model.transform(test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "roc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "roc_score = roc_evaluator.evaluate(predictions)\n",
    "print(\"Area under ROC = %g\" % roc_score)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "pr_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
    "pr_score = pr_evaluator.evaluate(predictions)\n",
    "print(\"Area under PR = %g\" % pr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a NaiveBayes instance\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")\n",
    "\n",
    "# Create a grid of parameters to search through\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.0, 0.01, 0.1, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Create an evaluator for multi-class classification\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "# Set up the TrainValidationSplit with cross-validation\n",
    "tvs = TrainValidationSplit(estimator=nb,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           trainRatio=0.8)  # 80% of the data will be used for training\n",
    "\n",
    "# Fit the model using the TrainValidationSplit\n",
    "tvs_model = tvs.fit(train_with_weights)\n",
    "\n",
    "# Get the best NaiveBayes model from the grid search\n",
    "best_nb_model = tvs_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction \n",
    "\n",
    "predictions = best_nb_model.transform(test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Create an evaluator for accuracy\n",
    "roc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "roc_score = roc_evaluator.evaluate(predictions)\n",
    "print(\"Area under ROC = %g\" % roc_score)\n",
    "\n",
    "# Create an evaluator for f1 score\n",
    "pr_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderPR\")\n",
    "pr_score = pr_evaluator.evaluate(predictions)\n",
    "print(\"Area under PR = %g\" % pr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
